{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 25 06:35:55 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:3B:00.0 Off |                  Off |\n",
      "| 31%   40C    P0             62W /  450W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:5E:00.0 Off |                  Off |\n",
      "| 32%   40C    P0             66W /  450W |       0MiB /  24564MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/'\n",
      "/home/syq/Fine tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syq/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 317, done.\u001b[K\n",
      "remote: Counting objects: 100% (317/317), done.\u001b[K\n",
      "remote: Compressing objects: 100% (256/256), done.\u001b[K\n",
      "remote: Total 317 (delta 78), reused 153 (delta 48), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (317/317), 8.97 MiB | 6.97 MiB/s, done.\n",
      "Resolving deltas: 100% (78/78), done.\n",
      "/home/syq/Fine tuning/LLaMA-Factory\n",
      "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/  MANIFEST.in     requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
      "CITATION.cff  \u001b[01;34mexamples\u001b[0m/    pyproject.toml  \u001b[01;34mscripts\u001b[0m/\n",
      "\u001b[01;34mdata\u001b[0m/         LICENSE      README.md       setup.py\n",
      "\u001b[01;34mdocker\u001b[0m/       Makefile     README_zh.md    \u001b[01;34msrc\u001b[0m/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syq/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.3.1\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.18.1\n",
      "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.3.1\n",
      "  Downloading torchaudio-2.3.1-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.3.1) (3.0.3)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.3.1\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m902.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m774.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:14\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m935.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /home/syq/.local/lib/python3.10/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m898.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m668.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.10.0 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.1.105 pillow-11.0.0 sympy-1.13.3 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1 triton-2.3.1\n",
      "\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///home/syq/Fine%20tuning/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<=4.46.1,>=4.41.2\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2.0.0\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trl<=0.9.6,>=0.8.6\n",
      "  Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas>=2.0.0\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets<=3.1.0,>=2.16.0\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting av\n",
      "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3/dist-packages (from llamafactory==0.9.2.dev0) (21.3)\n",
      "Collecting matplotlib>=3.7.0\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fire\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio<5.0.0,>=4.0.0\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m909.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tyro<0.9.0\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 KB\u001b[0m \u001b[31m911.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from llamafactory==0.9.2.dev0) (5.4.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic\n",
      "  Downloading pydantic-2.10.1-py3-none-any.whl (455 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.3/455.3 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft<=0.12.0,>=0.11.1\n",
      "  Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sse-starlette\n",
      "  Downloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
      "Collecting accelerate<=1.0.1,>=0.34.0\n",
      "  Downloading accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting liger-kernel\n",
      "  Downloading liger_kernel-0.4.2-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes>=0.39.0\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m862.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.1 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory==0.9.2.dev0) (2.3.1)\n",
      "Collecting huggingface-hub>=0.21.0\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/syq/.local/lib/python3.10/site-packages (from accelerate<=1.0.1,>=0.34.0->llamafactory==0.9.2.dev0) (6.1.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 KB\u001b[0m \u001b[31m964.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m964.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.11.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/syq/.local/lib/python3.10/site-packages (from datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.16.1)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tomlkit==0.12.0\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting semantic-version~=2.0\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typer<1.0,>=0.12\n",
      "  Downloading typer-0.13.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.24.1\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting orjson~=3.0\n",
      "  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (4.12.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/lib/python3/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.0.3)\n",
      "Collecting importlib-resources<7.0,>=1.3\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Collecting urllib3~=2.0\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow<11.0,>=8.0\n",
      "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markupsafe~=2.0 in /usr/lib/python3/dist-packages (from gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2.0.1)\n",
      "Collecting ruff>=0.2.2\n",
      "  Downloading ruff-0.8.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gradio-client==1.3.0\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart>=0.0.9\n",
      "  Downloading python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiofiles<24.0,>=22.0\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting anyio<5.0,>=3.0\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting websockets<13.0,>=10.0\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/syq/.local/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2024.10.0)\n",
      "Collecting starlette<0.42.0,>=0.40.0\n",
      "  Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m888.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/syq/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.4.7)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 KB\u001b[0m \u001b[31m688.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2022.1)\n",
      "Collecting pydantic-core==2.27.1\n",
      "  Downloading pydantic_core-2.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m726.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: networkx in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (3.4.2)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (2.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.9.2.dev0) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/syq/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.9.2.dev0) (12.6.85)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m618.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m870.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich>=11.1.0\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 KB\u001b[0m \u001b[31m740.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docstring-parser>=0.16\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting shtab>=1.5.6\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.0.3)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m687.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/syq/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5.0,>=3.0->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (3.3)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m821.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 KB\u001b[0m \u001b[31m886.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 KB\u001b[0m \u001b[31m888.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.4/319.4 KB\u001b[0m \u001b[31m959.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets<=3.1.0,>=2.16.0->llamafactory==0.9.2.dev0) (21.2.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx>=0.24.1->gradio<5.0.0,>=4.0.0->llamafactory==0.9.2.dev0) (2020.6.20)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m953.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 KB\u001b[0m \u001b[31m983.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 KB\u001b[0m \u001b[31m916.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/syq/.local/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.9.2.dev0) (1.3.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114262 sha256=bf5955c4688659937c17c11eee24aab5e563d941d3aba41ca5ad37f9664b75fc\n",
      "  Stored in directory: /home/syq/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
      "Successfully built fire\n",
      "Installing collected packages: sentencepiece, pydub, xxhash, websockets, urllib3, tzdata, tqdm, tomlkit, termcolor, sniffio, shtab, shellingham, semantic-version, safetensors, ruff, regex, python-multipart, pygments, pydantic-core, pyarrow, protobuf, propcache, pillow, orjson, numpy, multidict, mdurl, kiwisolver, importlib-resources, h11, fsspec, frozenlist, fonttools, ffmpy, einops, docstring-parser, dill, cycler, charset-normalizer, av, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, yarl, uvicorn, scipy, requests, pydantic, pandas, multiprocess, markdown-it-py, httpcore, fire, contourpy, anyio, aiosignal, tiktoken, starlette, rich, matplotlib, huggingface-hub, httpx, aiohttp, tyro, typer, tokenizers, sse-starlette, liger-kernel, gradio-client, fastapi, bitsandbytes, accelerate, transformers, gradio, datasets, trl, peft, llamafactory\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.0.0\n",
      "    Uninstalling pillow-11.0.0:\n",
      "      Successfully uninstalled pillow-11.0.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Running setup.py develop for llamafactory\n",
      "    \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "    \n",
      "    \u001b[31m×\u001b[0m \u001b[32mpython setup.py develop\u001b[0m did not run successfully.\n",
      "    \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "    \u001b[31m╰─>\u001b[0m \u001b[31m[32 lines of output]\u001b[0m\n",
      "    \u001b[31m   \u001b[0m running develop\n",
      "    \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/setuptools/command/easy_install.py:158: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "    \u001b[31m   \u001b[0m   warnings.warn(\n",
      "    \u001b[31m   \u001b[0m WARNING: The user site-packages directory is disabled.\n",
      "    \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    \u001b[31m   \u001b[0m   warnings.warn(\n",
      "    \u001b[31m   \u001b[0m error: can't create or remove files in install directory\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m The following error occurred while trying to add or remove files in the\n",
      "    \u001b[31m   \u001b[0m installation directory:\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m     [Errno 13] Permission denied: '/usr/local/lib/python3.10/dist-packages/test-easy-install-828797.write-test'\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m The installation directory you specified (via --install-dir, --prefix, or\n",
      "    \u001b[31m   \u001b[0m the distutils default setting) was:\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m     /usr/local/lib/python3.10/dist-packages/\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m Perhaps your account does not have write access to this directory?  If the\n",
      "    \u001b[31m   \u001b[0m installation directory is a system-owned directory, you may need to sign in\n",
      "    \u001b[31m   \u001b[0m as the administrator or \"root\" account.  If you do not have administrative\n",
      "    \u001b[31m   \u001b[0m access to this machine, you may wish to choose a different installation\n",
      "    \u001b[31m   \u001b[0m directory, preferably one that is listed in your PYTHONPATH environment\n",
      "    \u001b[31m   \u001b[0m variable.\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m For information on other options, you may wish to consult the\n",
      "    \u001b[31m   \u001b[0m documentation at:\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m   https://setuptools.pypa.io/en/latest/deprecated/easy_install.html\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m Please make the appropriate changes for your system and try again.\n",
      "    \u001b[31m   \u001b[0m \n",
      "    \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "    \n",
      "    \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpython setup.py develop\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m \u001b[31m[32 lines of output]\u001b[0m\n",
      "\u001b[31m   \u001b[0m running develop\n",
      "\u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/setuptools/command/easy_install.py:158: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "\u001b[31m   \u001b[0m   warnings.warn(\n",
      "\u001b[31m   \u001b[0m WARNING: The user site-packages directory is disabled.\n",
      "\u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "\u001b[31m   \u001b[0m   warnings.warn(\n",
      "\u001b[31m   \u001b[0m error: can't create or remove files in install directory\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m The following error occurred while trying to add or remove files in the\n",
      "\u001b[31m   \u001b[0m installation directory:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m     [Errno 13] Permission denied: '/usr/local/lib/python3.10/dist-packages/test-easy-install-828797.write-test'\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m The installation directory you specified (via --install-dir, --prefix, or\n",
      "\u001b[31m   \u001b[0m the distutils default setting) was:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m     /usr/local/lib/python3.10/dist-packages/\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Perhaps your account does not have write access to this directory?  If the\n",
      "\u001b[31m   \u001b[0m installation directory is a system-owned directory, you may need to sign in\n",
      "\u001b[31m   \u001b[0m as the administrator or \"root\" account.  If you do not have administrative\n",
      "\u001b[31m   \u001b[0m access to this machine, you may wish to choose a different installation\n",
      "\u001b[31m   \u001b[0m directory, preferably one that is listed in your PYTHONPATH environment\n",
      "\u001b[31m   \u001b[0m variable.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m For information on other options, you may wish to consult the\n",
      "\u001b[31m   \u001b[0m documentation at:\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m   https://setuptools.pypa.io/en/latest/deprecated/easy_install.html\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m Please make the appropriate changes for your system and try again.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "%cd /content/\n",
    "%rm -rf LLaMA-Factory\n",
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%ls\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\n",
    "!pip uninstall -y jax\n",
    "!pip install -e .[torch,bitsandbytes,liger-kernel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping vllm as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llamafactory[metrics]==0.7.1\n",
      "  Downloading llamafactory-0.7.1-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.5/164.5 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.2.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (3.9.2)\n",
      "Requirement already satisfied: trl>=0.8.1 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.9.6)\n",
      "Requirement already satisfied: fire in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.7.0)\n",
      "Requirement already satisfied: peft>=0.10.0 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from llamafactory[metrics]==0.7.1) (5.4.1)\n",
      "Requirement already satisfied: uvicorn in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.32.1)\n",
      "Requirement already satisfied: protobuf in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (5.28.3)\n",
      "Requirement already satisfied: pydantic in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (2.10.1)\n",
      "Requirement already satisfied: scipy in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (1.14.1)\n",
      "Requirement already satisfied: datasets>=2.14.3 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (3.1.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from llamafactory[metrics]==0.7.1) (21.3)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (4.46.1)\n",
      "Requirement already satisfied: fastapi in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.115.5)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (4.44.1)\n",
      "Requirement already satisfied: sse-starlette in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (2.1.3)\n",
      "Requirement already satisfied: einops in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.8.0)\n",
      "Requirement already satisfied: accelerate>=0.27.2 in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (1.0.1)\n",
      "Requirement already satisfied: jieba in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (1.0.3)\n",
      "Requirement already satisfied: nltk in /home/syq/.local/lib/python3.10/site-packages (from llamafactory[metrics]==0.7.1) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/syq/.local/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (0.26.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/syq/.local/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (2.3.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/syq/.local/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (0.4.5)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/syq/.local/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (1.26.4)\n",
      "Requirement already satisfied: psutil in /home/syq/.local/lib/python3.10/site-packages (from accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (6.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (2024.9.0)\n",
      "Requirement already satisfied: xxhash in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (3.11.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (18.0.0)\n",
      "Requirement already satisfied: pandas in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (2.2.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (0.70.16)\n",
      "Requirement already satisfied: filelock in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (3.16.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/syq/.local/lib/python3.10/site-packages (from datasets>=2.14.3->llamafactory[metrics]==0.7.1) (4.67.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/lib/python3/dist-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (2.0.1)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (2.10.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.13.1)\n",
      "Requirement already satisfied: pydub in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.25.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/lib/python3/dist-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (3.0.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (6.4.5)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.0.17)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (23.2.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (4.12.2)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (4.6.2.post1)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (2.2.3)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.8.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.12.0)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (3.10.12)\n",
      "Requirement already satisfied: ffmpy in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/syq/.local/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.27.2)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /home/syq/.local/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (12.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/syq/.local/lib/python3.10/site-packages (from fastapi->llamafactory[metrics]==0.7.1) (0.41.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/syq/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory[metrics]==0.7.1) (4.55.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/syq/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory[metrics]==0.7.1) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/syq/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory[metrics]==0.7.1) (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/syq/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory[metrics]==0.7.1) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/syq/.local/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory[metrics]==0.7.1) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.7.0->llamafactory[metrics]==0.7.1) (2.4.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/syq/.local/lib/python3.10/site-packages (from pydantic->llamafactory[metrics]==0.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/syq/.local/lib/python3.10/site-packages (from pydantic->llamafactory[metrics]==0.7.1) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/syq/.local/lib/python3.10/site-packages (from transformers>=4.37.2->llamafactory[metrics]==0.7.1) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/syq/.local/lib/python3.10/site-packages (from transformers>=4.37.2->llamafactory[metrics]==0.7.1) (2024.11.6)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /home/syq/.local/lib/python3.10/site-packages (from trl>=0.8.1->llamafactory[metrics]==0.7.1) (0.8.14)\n",
      "Requirement already satisfied: h11>=0.8 in /home/syq/.local/lib/python3.10/site-packages (from uvicorn->llamafactory[metrics]==0.7.1) (0.14.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from uvicorn->llamafactory[metrics]==0.7.1) (8.0.3)\n",
      "Requirement already satisfied: termcolor in /home/syq/.local/lib/python3.10/site-packages (from fire->llamafactory[metrics]==0.7.1) (2.5.0)\n",
      "Requirement already satisfied: joblib in /home/syq/.local/lib/python3.10/site-packages (from nltk->llamafactory[metrics]==0.7.1) (1.4.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge-chinese->llamafactory[metrics]==0.7.1) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/syq/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/syq/.local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (1.18.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (0.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (2.4.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/syq/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (1.5.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /home/syq/.local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (1.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/syq/.local/lib/python3.10/site-packages (from pandas->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/syq/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.3->llamafactory[metrics]==0.7.1) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (2.20.5)\n",
      "Requirement already satisfied: sympy in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (1.13.3)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (2.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/syq/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (12.6.85)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/syq/.local/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (13.9.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/syq/.local/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (1.5.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /home/syq/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory[metrics]==0.7.1) (1.7.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /home/syq/.local/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.1->llamafactory[metrics]==0.7.1) (0.16)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/syq/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/syq/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (3.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/syq/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.27.2->llamafactory[metrics]==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/syq/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory[metrics]==0.7.1) (0.1.2)\n",
      "Installing collected packages: llamafactory\n",
      "Successfully installed llamafactory-0.7.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate==0.30.1\n",
      "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /home/syq/.local/lib/python3.10/site-packages (from accelerate==0.30.1) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/syq/.local/lib/python3.10/site-packages (from accelerate==0.30.1) (2.3.1)\n",
      "Requirement already satisfied: psutil in /home/syq/.local/lib/python3.10/site-packages (from accelerate==0.30.1) (6.1.0)\n",
      "Requirement already satisfied: huggingface-hub in /home/syq/.local/lib/python3.10/site-packages (from accelerate==0.30.1) (0.26.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate==0.30.1) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate==0.30.1) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/syq/.local/lib/python3.10/site-packages (from accelerate==0.30.1) (1.26.4)\n",
      "Requirement already satisfied: sympy in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.10.0->accelerate==0.30.1) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (12.1.105)\n",
      "Requirement already satisfied: fsspec in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (11.4.5.107)\n",
      "Requirement already satisfied: filelock in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (4.12.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (2.20.5)\n",
      "Requirement already satisfied: networkx in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/syq/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/syq/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.30.1) (12.6.85)\n",
      "Requirement already satisfied: requests in /home/syq/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/syq/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30.1) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/syq/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate==0.30.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/syq/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate==0.30.1) (2020.6.20)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/syq/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30.1) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.0.1\n",
      "    Uninstalling accelerate-1.0.1:\n",
      "      Successfully uninstalled accelerate-1.0.1\n",
      "Successfully installed accelerate-0.30.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y vllm\n",
    "!pip install llamafactory[metrics]==0.7.1\n",
    "!pip install accelerate==0.30.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "| Welcome to LLaMA Factory, version 0.7.1                |\n",
      "|                                                        |\n",
      "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 查看llamafactory是否安装成功\n",
    "!llamafactory-cli version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu121'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查torch和显卡\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Baichuan2-7B-Chat'...\n",
      "remote: Enumerating objects: 115, done.\u001b[K\n",
      "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 115 (delta 16), reused 14 (delta 7), pack-reused 78\u001b[K\n",
      "Receiving objects: 100% (115/115), 468.12 KiB | 3.49 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n",
      "^C\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n",
      "\n",
      "Exiting because of \"interrupt\" signal.\n"
     ]
    }
   ],
   "source": [
    "# 下载模型\n",
    "!git clone https://www.modelscope.cn/baichuan-inc/Baichuan2-7B-Chat.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: modelscope in /home/syq/.local/lib/python3.10/site-packages (1.20.1)\n",
      "Requirement already satisfied: requests>=2.25 in /home/syq/.local/lib/python3.10/site-packages (from modelscope) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26 in /home/syq/.local/lib/python3.10/site-packages (from modelscope) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/syq/.local/lib/python3.10/site-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.25->modelscope) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/syq/.local/lib/python3.10/site-packages (from requests>=2.25->modelscope) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.25->modelscope) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 syq syq 14G Nov 25 10:02 /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n",
      "Model loaded successfully!\n",
      "Generation config loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 测试模型是否导入成功\n",
    "from modelscope import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# 本地模型路径\n",
    "model_dir = \"/home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat\"\n",
    "\n",
    "# 加载分词器\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    "    )\n",
    "    print(\"Tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading tokenizer:\", e)\n",
    "\n",
    "# 加载模型\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading model:\", e)\n",
    "\n",
    "# 加载生成配置\n",
    "try:\n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_dir)\n",
    "    print(\"Generation config loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading generation config:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/syq/.local/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "11/26/2024 04:56:50 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-26 04:56:50,132 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-26 04:56:50,132 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-26 04:56:50,132 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-26 04:56:50,132 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2209] 2024-11-26 04:56:50,132 >> loading file tokenizer.json\n",
      "11/26/2024 04:56:50 - INFO - llamafactory.data.loader - Loading dataset train.json...\n",
      "Converting format of dataset (num_proc=16): 100%|█| 1000/1000 [00:00<00:00, 3767\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:00<00:00, 2481\n",
      "input_ids:\n",
      "[195, 2169, 11953, 2665, 40094, 65, 13371, 20374, 5875, 13449, 66, 5, 2665, 40094, 70, 3939, 92403, 8917, 18526, 93189, 3770, 26373, 3939, 32862, 57315, 65, 2999, 71770, 2926, 31730, 93282, 92419, 3979, 78, 4830, 9464, 79, 66, 4850, 65, 20455, 4005, 35466, 29722, 92492, 23934, 92333, 19007, 9078, 65, 71150, 92338, 92335, 92336, 92358, 92381, 92551, 25320, 11619, 66, 196, 8917, 18526, 92347, 92338, 92335, 6335, 15964, 3939, 32862, 57315, 66, 2999, 17366, 20897, 92355, 5670, 3581, 92364, 3328, 20839, 65, 9193, 7884, 92881, 12800, 45207, 50623, 66, 2776, 65, 2999, 1741, 3940, 20348, 47558, 92333, 66, 92355, 38179, 92333, 11717, 65, 16608, 4005, 25902, 9805, 2353, 18340, 66, 42295, 7444, 92410, 16869, 2353, 26182, 93143, 65, 73680, 2999, 1741, 4459, 2071, 4008, 66, 61427, 8348, 1537, 92594, 44802, 36246, 11774, 92547, 66, 5, 5, 66983, 65, 8917, 18526, 6178, 5546, 66, 9440, 62926, 92385, 47058, 6179, 65, 2781, 11471, 3939, 1998, 65, 2783, 2848, 1937, 15532, 3531, 66, 2999, 2783, 3647, 13273, 11909, 65, 9440, 3939, 92385, 15401, 3581, 19581, 3833, 5051, 5442, 65, 92594, 2895, 12800, 36997, 15401, 93133, 66, 5, 5, 26971, 92424, 65, 8917, 18526, 4618, 15677, 19007, 1754, 65, 4451, 10879, 50377, 93143, 13588, 66, 27864, 6593, 92343, 54349, 76343, 65, 92453, 13438, 3546, 49175, 65, 28638, 2999, 9893, 9585, 6912, 66, 4850, 65, 82070, 65, 9440, 92338, 92335, 92336, 92358, 92381, 92551, 6266, 25320, 11619, 66, 2999, 80290, 42801, 3136, 7944, 9440, 3939, 92385, 15401, 3581, 35762, 5442, 65, 13825, 17954, 38148, 33435, 63945, 10378, 36350, 39803, 5232, 66, 3979, 78, 4830, 9464, 79, 5089, 84839, 8917, 18526, 92372, 46057, 13767, 92385, 34869, 65, 92435, 4776, 5073, 12532, 92492, 6932, 92333, 3939, 92403, 14762, 9464, 7486, 8365, 66, 2]\n",
      "inputs:\n",
      " <reserved_106>根据提供的文章摘要，生成一篇完整的文章。\n",
      "文章摘要：数学家John Nash曾是一个天才数学家和经济学家，他的生平故事曾被拍成电影《美丽心灵》。然而，他也经历了一段漫长而艰辛的心理健康之路，最终在2015年因车祸去世。<reserved_107>John Nash是20世纪著名的数学家和经济学家。他的卓越才华在多个领域中表现出色，并在学术界获得了广泛的赞誉。不过，他的生活并不一直是平坦的。在职业生涯的早期，他就经历了一次严重的精神崩溃。他被诊断出患有精神分裂症，这使得他的生活变得非常困难。他开始失去工作并陷入了长期的孤独感。\n",
      "\n",
      "尽管如此，John Nash并没有放弃。他在接受治疗和药物治疗的同时，继续致力于数学研究，努力保持自己的优良状态。他的努力最终取得了回报，他在数学和经济学领域做出了许多重要的贡献，并因此获得了诺贝尔经济学奖。\n",
      "\n",
      "几十年后，John Nash再次经历了心理健康问题，这次是由老年痴呆症引起的。在与妻子Alicia的支持下，他找到了新的治疗方法，这让他的病情得到了缓解。然而，不幸的是，他在2015年因一场车祸去世。他的逝去让人们更加珍惜他在数学和经济学领域做出的贡献，同时也对他长期以来坚韧不拔的精神给予了极高的评价。电影《美丽心灵》让我们回顾了John Nash生平的辉煌和曲折，也展示了一个勇敢而自信的数学家对抗心灵痛苦的故事。</s>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 8917, 18526, 92347, 92338, 92335, 6335, 15964, 3939, 32862, 57315, 66, 2999, 17366, 20897, 92355, 5670, 3581, 92364, 3328, 20839, 65, 9193, 7884, 92881, 12800, 45207, 50623, 66, 2776, 65, 2999, 1741, 3940, 20348, 47558, 92333, 66, 92355, 38179, 92333, 11717, 65, 16608, 4005, 25902, 9805, 2353, 18340, 66, 42295, 7444, 92410, 16869, 2353, 26182, 93143, 65, 73680, 2999, 1741, 4459, 2071, 4008, 66, 61427, 8348, 1537, 92594, 44802, 36246, 11774, 92547, 66, 5, 5, 66983, 65, 8917, 18526, 6178, 5546, 66, 9440, 62926, 92385, 47058, 6179, 65, 2781, 11471, 3939, 1998, 65, 2783, 2848, 1937, 15532, 3531, 66, 2999, 2783, 3647, 13273, 11909, 65, 9440, 3939, 92385, 15401, 3581, 19581, 3833, 5051, 5442, 65, 92594, 2895, 12800, 36997, 15401, 93133, 66, 5, 5, 26971, 92424, 65, 8917, 18526, 4618, 15677, 19007, 1754, 65, 4451, 10879, 50377, 93143, 13588, 66, 27864, 6593, 92343, 54349, 76343, 65, 92453, 13438, 3546, 49175, 65, 28638, 2999, 9893, 9585, 6912, 66, 4850, 65, 82070, 65, 9440, 92338, 92335, 92336, 92358, 92381, 92551, 6266, 25320, 11619, 66, 2999, 80290, 42801, 3136, 7944, 9440, 3939, 92385, 15401, 3581, 35762, 5442, 65, 13825, 17954, 38148, 33435, 63945, 10378, 36350, 39803, 5232, 66, 3979, 78, 4830, 9464, 79, 5089, 84839, 8917, 18526, 92372, 46057, 13767, 92385, 34869, 65, 92435, 4776, 5073, 12532, 92492, 6932, 92333, 3939, 92403, 14762, 9464, 7486, 8365, 66, 2]\n",
      "labels:\n",
      " John Nash是20世纪著名的数学家和经济学家。他的卓越才华在多个领域中表现出色，并在学术界获得了广泛的赞誉。不过，他的生活并不一直是平坦的。在职业生涯的早期，他就经历了一次严重的精神崩溃。他被诊断出患有精神分裂症，这使得他的生活变得非常困难。他开始失去工作并陷入了长期的孤独感。\n",
      "\n",
      "尽管如此，John Nash并没有放弃。他在接受治疗和药物治疗的同时，继续致力于数学研究，努力保持自己的优良状态。他的努力最终取得了回报，他在数学和经济学领域做出了许多重要的贡献，并因此获得了诺贝尔经济学奖。\n",
      "\n",
      "几十年后，John Nash再次经历了心理健康问题，这次是由老年痴呆症引起的。在与妻子Alicia的支持下，他找到了新的治疗方法，这让他的病情得到了缓解。然而，不幸的是，他在2015年因一场车祸去世。他的逝去让人们更加珍惜他在数学和经济学领域做出的贡献，同时也对他长期以来坚韧不拔的精神给予了极高的评价。电影《美丽心灵》让我们回顾了John Nash生平的辉煌和曲折，也展示了一个勇敢而自信的数学家对抗心灵痛苦的故事。</s>\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 04:56:52,816 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 04:56:52,818 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 04:56:52,819 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"_name_or_path\": \"/home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "[INFO|modeling_utils.py:3934] 2024-11-26 04:56:52,852 >> loading weights file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1670] 2024-11-26 04:56:52,878 >> Instantiating BaichuanForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1096] 2024-11-26 04:56:52,879 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4800] 2024-11-26 04:56:59,294 >> All model checkpoint weights were used when initializing BaichuanForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4808] 2024-11-26 04:56:59,295 >> All the weights of BaichuanForCausalLM were initialized from the model checkpoint at /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BaichuanForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1049] 2024-11-26 04:56:59,297 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/generation_config.json\n",
      "[INFO|configuration_utils.py:1096] 2024-11-26 04:56:59,297 >> Generate config GenerationConfig {\n",
      "  \"assistant_token_id\": 196,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 5,\n",
      "  \"top_p\": 0.85,\n",
      "  \"user_token_id\": 195\n",
      "}\n",
      "\n",
      "11/26/2024 04:56:59 - WARNING - llamafactory.model.utils.checkpointing - You are using the old GC format, some features (e.g. BAdam) will be invalid.\n",
      "11/26/2024 04:56:59 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "11/26/2024 04:56:59 - INFO - llamafactory.model.utils.attention - Using vanilla attention implementation.\n",
      "11/26/2024 04:56:59 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "11/26/2024 04:56:59 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "11/26/2024 04:56:59 - INFO - llamafactory.model.utils.misc - Found linear modules: gate_proj,W_pack,down_proj,up_proj,o_proj\n",
      "11/26/2024 04:56:59 - INFO - llamafactory.model.loader - trainable params: 17891328 || all params: 7523864576 || trainable%: 0.2378\n",
      "/home/syq/.local/lib/python3.10/site-packages/llamafactory/train/sft/trainer.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n",
      "[INFO|trainer.py:698] 2024-11-26 04:56:59,818 >> Using auto half precision backend\n",
      "11/26/2024 04:56:59 - WARNING - llamafactory.extras.callbacks - Previous trainer log in this folder will be deleted.\n",
      "[INFO|trainer.py:2313] 2024-11-26 04:57:00,025 >> ***** Running training *****\n",
      "[INFO|trainer.py:2314] 2024-11-26 04:57:00,025 >>   Num examples = 900\n",
      "[INFO|trainer.py:2315] 2024-11-26 04:57:00,025 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2316] 2024-11-26 04:57:00,025 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2319] 2024-11-26 04:57:00,025 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2320] 2024-11-26 04:57:00,025 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:2321] 2024-11-26 04:57:00,025 >>   Total optimization steps = 675\n",
      "[INFO|trainer.py:2322] 2024-11-26 04:57:00,028 >>   Number of trainable parameters = 17,891,328\n",
      "  0%|                                                   | 0/675 [00:00<?, ?it/s]/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 3.8102, 'grad_norm': 1.624354600906372, 'learning_rate': 4.975855242337838e-05, 'epoch': 0.22}\n",
      "  7%|███                                       | 50/675 [00:28<06:13,  1.67it/s][INFO|trainer.py:4117] 2024-11-26 04:57:28,716 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 04:57:28,716 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 04:57:28,716 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.91it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.33it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.12it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.76it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.48it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.35it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.26it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.12it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.09it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.94it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.57it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.72it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.60it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.73it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.82it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.88it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.82it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.90it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.88it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.81it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.87it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.95it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.96it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.99it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.90it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.94it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 18.00it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.96it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.92it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.91it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.97it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.02it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.03it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.91it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.91it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.87it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.87it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.94it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.98it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 18.02it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.06it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.05it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 17.95it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3780486583709717, 'eval_runtime': 5.6005, 'eval_samples_per_second': 17.855, 'eval_steps_per_second': 17.855, 'epoch': 0.22}\n",
      "  7%|███                                       | 50/675 [00:34<06:13,  1.67it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 17.88it/s]\u001b[A\n",
      "{'loss': 3.2273, 'grad_norm': 2.303980588912964, 'learning_rate': 4.822671293082922e-05, 'epoch': 0.44}\n",
      " 15%|██████                                   | 100/675 [01:02<05:27,  1.76it/s][INFO|trainer.py:4117] 2024-11-26 04:58:02,291 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 04:58:02,291 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 04:58:02,291 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 25.94it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.44it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.20it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.84it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.54it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.39it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.28it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.13it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.11it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.96it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.82it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.91it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.74it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.83it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.89it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.94it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.92it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.97it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.93it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.84it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.91it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.99it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 18.01it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.96it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 18.00it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.07it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.99it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 18.03it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 18.00it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 18.04it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.08it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.09it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.89it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.90it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.87it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.94it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 18.00it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 18.03it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 18.07it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.09it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.10it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.10it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3263897895812988, 'eval_runtime': 5.5816, 'eval_samples_per_second': 17.916, 'eval_steps_per_second': 17.916, 'epoch': 0.44}\n",
      " 15%|██████                                   | 100/675 [01:07<05:27,  1.76it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.11it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2024-11-26 04:58:07,873 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-100\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 04:58:07,895 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 04:58:07,895 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 04:58:08,056 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 04:58:08,056 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-100/special_tokens_map.json\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 3.1301, 'grad_norm': 2.630182981491089, 'learning_rate': 4.536545258887028e-05, 'epoch': 0.67}\n",
      " 22%|█████████                                | 150/675 [01:36<05:10,  1.69it/s][INFO|trainer.py:4117] 2024-11-26 04:58:36,645 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 04:58:36,645 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 04:58:36,645 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.94it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.66it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.31it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.90it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.55it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.26it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.12it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.09it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.95it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.86it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.94it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.75it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.83it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.89it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.93it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.90it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.96it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.92it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.83it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.90it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.96it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.97it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 18.00it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 18.00it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 18.01it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.95it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.96it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.93it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.96it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 18.00it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.04it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.04it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.91it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.91it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.87it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.93it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.99it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 18.01it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 18.04it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 17.98it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.00it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3092151880264282, 'eval_runtime': 5.586, 'eval_samples_per_second': 17.902, 'eval_steps_per_second': 17.902, 'epoch': 0.67}\n",
      " 22%|█████████                                | 150/675 [01:42<05:10,  1.69it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.03it/s]\u001b[A\n",
      "{'loss': 3.2757, 'grad_norm': 9.30089282989502, 'learning_rate': 4.133854055267725e-05, 'epoch': 0.89}\n",
      " 30%|████████████▏                            | 200/675 [02:10<04:25,  1.79it/s][INFO|trainer.py:4117] 2024-11-26 04:59:10,327 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 04:59:10,327 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 04:59:10,327 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.98it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.73it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.36it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.94it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.60it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.44it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.31it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.14it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.11it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.96it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.85it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.93it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.74it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.81it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.87it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.91it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.89it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.95it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.91it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.84it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.83it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.91it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.96it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 18.01it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 18.01it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.05it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.08it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 18.01it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 18.02it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.06it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.06it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.93it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.93it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.88it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.95it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 18.00it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 18.02it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 18.05it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3026916980743408, 'eval_runtime': 5.5804, 'eval_samples_per_second': 17.92, 'eval_steps_per_second': 17.92, 'epoch': 0.89}\n",
      " 30%|████████████▏                            | 200/675 [02:15<04:25,  1.79it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.08it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2024-11-26 04:59:15,908 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-200\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 04:59:15,928 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 04:59:15,929 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 04:59:16,099 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 04:59:16,099 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-200/special_tokens_map.json\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 3.0502, 'grad_norm': 3.244025945663452, 'learning_rate': 3.637646405630673e-05, 'epoch': 1.11}\n",
      " 37%|███████████████▏                         | 250/675 [02:44<03:54,  1.81it/s][INFO|trainer.py:4117] 2024-11-26 04:59:44,563 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 04:59:44,563 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 04:59:44,563 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 27.05it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.69it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.31it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.90it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.56it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.39it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.27it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.13it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.09it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.93it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.82it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.89it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.72it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.79it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.84it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.88it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.66it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.70it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.68it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.78it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.87it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.92it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.94it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.95it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.95it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.87it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.86it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.82it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.88it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.93it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.96it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.99it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.03it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 17.99it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.00it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3055169582366943, 'eval_runtime': 5.5971, 'eval_samples_per_second': 17.866, 'eval_steps_per_second': 17.866, 'epoch': 1.11}\n",
      " 37%|███████████████▏                         | 250/675 [02:50<03:54,  1.81it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      "{'loss': 2.8873, 'grad_norm': 3.0226945877075195, 'learning_rate': 3.076323607956723e-05, 'epoch': 1.33}\n",
      " 44%|██████████████████▏                      | 300/675 [03:18<03:28,  1.80it/s][INFO|trainer.py:4117] 2024-11-26 05:00:18,080 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:00:18,080 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:00:18,080 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.97it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.70it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.32it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.89it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.55it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.40it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.28it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.12it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.09it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.93it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.83it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.90it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.72it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.79it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.85it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.88it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.82it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.89it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.85it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.78it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.85it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.92it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.94it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.01it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.04it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.91it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.96it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.94it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.97it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.88it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.86it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.81it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.88it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.93it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.95it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.98it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 17.99it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.00it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2907737493515015, 'eval_runtime': 5.5926, 'eval_samples_per_second': 17.881, 'eval_steps_per_second': 17.881, 'epoch': 1.33}\n",
      " 44%|██████████████████▏                      | 300/675 [03:23<03:28,  1.80it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2024-11-26 05:00:23,673 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-300\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 05:00:23,693 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 05:00:23,694 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 05:00:23,866 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 05:00:23,867 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-300/special_tokens_map.json\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 2.8867, 'grad_norm': 2.988063335418701, 'learning_rate': 2.482013937680245e-05, 'epoch': 1.56}\n",
      " 52%|█████████████████████▎                   | 350/675 [03:52<02:59,  1.81it/s][INFO|trainer.py:4117] 2024-11-26 05:00:52,493 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:00:52,493 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:00:52,493 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.93it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.69it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.35it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.94it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.60it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.43it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.30it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.15it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.12it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.96it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.86it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.92it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.73it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.80it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.86it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.90it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.87it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.93it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.90it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.82it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.88it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.96it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.99it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 18.02it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 18.03it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.04it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.08it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.99it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 18.02it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.99it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 18.03it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.07it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.07it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.93it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.92it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.87it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.92it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.98it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 18.01it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 18.03it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.07it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.08it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.07it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2996684312820435, 'eval_runtime': 5.5795, 'eval_samples_per_second': 17.923, 'eval_steps_per_second': 17.923, 'epoch': 1.56}\n",
      " 52%|█████████████████████▎                   | 350/675 [03:58<02:59,  1.81it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.09it/s]\u001b[A\n",
      "{'loss': 2.65, 'grad_norm': 4.453317642211914, 'learning_rate': 1.8887337306049473e-05, 'epoch': 1.78}\n",
      " 59%|████████████████████████▎                | 400/675 [04:26<02:35,  1.77it/s][INFO|trainer.py:4117] 2024-11-26 05:01:26,037 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:01:26,037 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:01:26,037 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.92it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.68it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.30it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.89it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.56it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.39it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.27it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.11it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.08it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.93it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.83it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.89it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.71it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.77it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.83it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.86it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.84it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.90it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.87it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.79it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.92it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.94it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.93it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.95it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.92it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.95it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.93it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.96it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 17.98it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 17.99it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.86it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.86it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.81it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.87it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.93it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.95it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.99it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.02it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.03it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3108917474746704, 'eval_runtime': 5.5947, 'eval_samples_per_second': 17.874, 'eval_steps_per_second': 17.874, 'epoch': 1.78}\n",
      " 59%|████████████████████████▎                | 400/675 [04:31<02:35,  1.77it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2024-11-26 05:01:31,633 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-400\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 05:01:31,653 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 05:01:31,653 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 05:01:31,826 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 05:01:31,826 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-400/special_tokens_map.json\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 2.8875, 'grad_norm': 4.701020240783691, 'learning_rate': 1.3304403994382125e-05, 'epoch': 2.0}\n",
      " 67%|███████████████████████████▎             | 450/675 [04:59<02:03,  1.82it/s][INFO|trainer.py:4117] 2024-11-26 05:01:59,846 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:01:59,846 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:01:59,846 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.90it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.63it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.28it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.88it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.54it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.38it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.26it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.13it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.10it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.94it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.83it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.89it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.71it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.80it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.84it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.88it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.92it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.89it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.79it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.85it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.92it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.94it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.97it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.97it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.01it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.95it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.99it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.96it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 18.00it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.03it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.03it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.90it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.90it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.85it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.91it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.91it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.93it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.97it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.03it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2994799613952637, 'eval_runtime': 5.5906, 'eval_samples_per_second': 17.887, 'eval_steps_per_second': 17.887, 'epoch': 2.0}\n",
      " 67%|███████████████████████████▎             | 450/675 [05:05<02:03,  1.82it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.05it/s]\u001b[A\n",
      "{'loss': 2.3232, 'grad_norm': 4.157318592071533, 'learning_rate': 8.390888228901615e-06, 'epoch': 2.22}\n",
      " 74%|██████████████████████████████▎          | 500/675 [05:33<01:40,  1.74it/s][INFO|trainer.py:4117] 2024-11-26 05:02:33,867 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:02:33,867 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:02:33,867 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.96it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.70it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.33it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.91it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.57it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.41it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.30it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.14it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.10it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.95it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.84it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.90it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.72it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.79it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.86it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.90it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.91it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.88it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.80it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.93it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.95it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.00it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.97it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.94it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.02it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.90it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.88it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.83it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.90it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.96it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.99it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 18.02it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.05it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3080406188964844, 'eval_runtime': 5.5883, 'eval_samples_per_second': 17.894, 'eval_steps_per_second': 17.894, 'epoch': 2.22}\n",
      " 74%|██████████████████████████████▎          | 500/675 [05:39<01:40,  1.74it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.05it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2024-11-26 05:02:39,456 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-500\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 05:02:39,476 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 05:02:39,477 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 05:02:39,632 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 05:02:39,632 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-500/special_tokens_map.json\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 2.3211, 'grad_norm': 5.621159076690674, 'learning_rate': 4.428023532491138e-06, 'epoch': 2.44}\n",
      " 81%|█████████████████████████████████▍       | 550/675 [06:07<01:10,  1.78it/s][INFO|trainer.py:4117] 2024-11-26 05:03:07,692 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:03:07,692 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:03:07,692 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.96it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.69it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.30it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.89it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.55it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.39it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.27it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.12it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.08it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.92it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.83it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.89it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.70it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.77it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.83it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.87it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.84it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.89it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.78it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.93it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.95it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 18.01it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.03it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.93it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.97it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.94it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.00it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.87it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.86it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.81it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.88it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.93it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.95it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.99it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.02it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.02it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3100262880325317, 'eval_runtime': 5.5923, 'eval_samples_per_second': 17.882, 'eval_steps_per_second': 17.882, 'epoch': 2.44}\n",
      " 81%|█████████████████████████████████▍       | 550/675 [06:13<01:10,  1.78it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.03it/s]\u001b[A\n",
      "{'loss': 2.7926, 'grad_norm': 5.295572757720947, 'learning_rate': 1.6856472040532923e-06, 'epoch': 2.67}\n",
      " 89%|████████████████████████████████████▍    | 600/675 [06:41<00:41,  1.81it/s][INFO|trainer.py:4117] 2024-11-26 05:03:41,401 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:03:41,402 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:03:41,402 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.98it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.66it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.29it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.88it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.54it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.37it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.26it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.10it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.06it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.91it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.81it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.88it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.70it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.78it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.85it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.89it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.92it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.88it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.80it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.87it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.92it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.94it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.99it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 17.98it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.94it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.95it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.98it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.03it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.03it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.90it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.88it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.83it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.90it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.96it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.98it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.96it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 17.99it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.312224268913269, 'eval_runtime': 5.5927, 'eval_samples_per_second': 17.88, 'eval_steps_per_second': 17.88, 'epoch': 2.67}\n",
      " 89%|████████████████████████████████████▍    | 600/675 [06:46<00:41,  1.81it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.04it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3801] 2024-11-26 05:03:46,995 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-600\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 05:03:47,015 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 05:03:47,016 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 05:03:47,171 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 05:03:47,171 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-600/special_tokens_map.json\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "{'loss': 2.7098, 'grad_norm': 3.2856078147888184, 'learning_rate': 2.093378016430364e-07, 'epoch': 2.89}\n",
      " 96%|███████████████████████████████████████▍ | 650/675 [07:15<00:13,  1.81it/s][INFO|trainer.py:4117] 2024-11-26 05:04:15,167 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:04:15,167 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:04:15,167 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:00<00:03, 26.90it/s]\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:00<00:04, 20.65it/s]\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:00<00:04, 19.27it/s]\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:00<00:04, 18.85it/s]\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:00<00:04, 18.51it/s]\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:00<00:04, 18.35it/s]\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:00<00:04, 18.22it/s]\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:01<00:04, 18.07it/s]\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:01<00:04, 18.03it/s]\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:01<00:04, 17.88it/s]\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:01<00:04, 17.79it/s]\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:01<00:04, 17.87it/s]\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:01<00:04, 17.68it/s]\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:01<00:03, 17.75it/s]\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:01<00:03, 17.81it/s]\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:01<00:03, 17.85it/s]\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:02<00:03, 17.83it/s]\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [00:02<00:03, 17.89it/s]\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [00:02<00:03, 17.86it/s]\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [00:02<00:03, 17.78it/s]\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [00:02<00:03, 17.85it/s]\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [00:02<00:02, 17.92it/s]\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [00:02<00:02, 17.94it/s]\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [00:02<00:02, 17.98it/s]\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [00:03<00:02, 17.99it/s]\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [00:03<00:02, 18.02it/s]\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [00:03<00:02, 17.97it/s]\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [00:03<00:01, 17.92it/s]\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [00:03<00:01, 17.96it/s]\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [00:03<00:01, 17.93it/s]\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [00:03<00:01, 17.97it/s]\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [00:04<00:01, 18.01it/s]\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [00:04<00:01, 18.00it/s]\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [00:04<00:01, 17.86it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [00:04<00:01, 17.84it/s]\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [00:04<00:01, 17.79it/s]\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [00:04<00:00, 17.86it/s]\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [00:04<00:00, 17.92it/s]\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [00:04<00:00, 17.95it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [00:04<00:00, 17.97it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [00:05<00:00, 18.00it/s]\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [00:05<00:00, 18.02it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [00:05<00:00, 18.02it/s]\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [00:05<00:00, 18.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3129652738571167, 'eval_runtime': 5.5975, 'eval_samples_per_second': 17.865, 'eval_steps_per_second': 17.865, 'epoch': 2.89}\n",
      " 96%|███████████████████████████████████████▍ | 650/675 [07:20<00:13,  1.81it/s]\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.03it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 675/675 [07:34<00:00,  1.72it/s][INFO|trainer.py:3801] 2024-11-26 05:04:34,973 >> Saving model checkpoint to ./saves/baichuan/lora/sft/checkpoint-675\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 05:04:34,991 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 05:04:34,992 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 05:04:35,146 >> tokenizer config file saved in ./saves/baichuan/lora/sft/checkpoint-675/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 05:04:35,147 >> Special tokens file saved in ./saves/baichuan/lora/sft/checkpoint-675/special_tokens_map.json\n",
      "[INFO|trainer.py:2584] 2024-11-26 05:04:35,366 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2822] 2024-11-26 05:04:35,366 >> Loading best model from ./saves/baichuan/lora/sft/checkpoint-300 (score: 1.2907737493515015).\n",
      "{'train_runtime': 455.4122, 'train_samples_per_second': 5.929, 'train_steps_per_second': 1.482, 'train_loss': 2.905831886574074, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████████| 675/675 [07:35<00:00,  1.48it/s]\n",
      "[INFO|trainer.py:3801] 2024-11-26 05:04:35,442 >> Saving model checkpoint to ./saves/baichuan/lora/sft\n",
      "[INFO|configuration_utils.py:677] 2024-11-26 05:04:35,460 >> loading configuration file /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat/config.json\n",
      "[INFO|configuration_utils.py:746] 2024-11-26 05:04:35,461 >> Model config BaichuanConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"architectures\": [\n",
      "    \"BaichuanForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_baichuan.BaichuanConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_baichuan.BaichuanForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_max_length\": 4096,\n",
      "  \"model_type\": \"baichuan\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"BaichuanTokenizer\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 125696,\n",
      "  \"z_loss_weight\": 0\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2646] 2024-11-26 05:04:35,612 >> tokenizer config file saved in ./saves/baichuan/lora/sft/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2655] 2024-11-26 05:04:35,612 >> Special tokens file saved in ./saves/baichuan/lora/sft/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 14913127GF\n",
      "  train_loss               =     2.9058\n",
      "  train_runtime            = 0:07:35.41\n",
      "  train_samples_per_second =      5.929\n",
      "  train_steps_per_second   =      1.482\n",
      "Figure saved at: ./saves/baichuan/lora/sft/training_loss.png\n",
      "Figure saved at: ./saves/baichuan/lora/sft/training_eval_loss.png\n",
      "[INFO|trainer.py:4117] 2024-11-26 05:04:35,841 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4119] 2024-11-26 05:04:35,841 >>   Num examples = 100\n",
      "[INFO|trainer.py:4122] 2024-11-26 05:04:35,841 >>   Batch size = 1\n",
      "/home/syq/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 100/100 [00:05<00:00, 18.07it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     1.2908\n",
      "  eval_runtime            = 0:00:05.60\n",
      "  eval_samples_per_second =     17.842\n",
      "  eval_steps_per_second   =     17.842\n",
      "[INFO|modelcard.py:449] 2024-11-26 05:04:41,464 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train \\\n",
    "    --model_name_or_path /home/syq/fine_tuning/test1/LLaMA-Factory/Baichuan2-7B-Chat \\\n",
    "    --dataset train \\\n",
    "    --dataset_dir /home/syq/fine_tuning/test1/LLaMA-Factory/data \\\n",
    "    --template baichuan2 \\\n",
    "    --finetuning_type lora \\\n",
    "    --output_dir ./saves/baichuan/lora/sft \\\n",
    "    --overwrite_cache \\\n",
    "    --overwrite_output_dir \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --logging_steps 100 \\\n",
    "    --warmup_steps 500 \\\n",
    "    --save_steps 1000 \\\n",
    "    --eval_steps 1000 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --load_best_model_at_end \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --val_size 0.1 \\\n",
    "    --plot_loss \\\n",
    "    --fp16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
